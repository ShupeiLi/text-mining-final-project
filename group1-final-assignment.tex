% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.21 of 2022/01/12
%
\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encondings may result in incorrect characters.
%
\usepackage{graphicx}
% Used for displaying a sample figure. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}

\begin{document}
%
\title{A BERT-Based Ensemble Learning Approach for Sentiment Classification in Twitter\thanks{Text Mining, Master CS, Fall 2022, Leiden, the Netherlands}}
%
\titlerunning{BERT-Based Ensemble Learning}
% If the paper title is too long for the running head, you can set
% an abbreviated paper title here
%
\author{Shupei Li\and
Ziyi Xu}
%
\authorrunning{S. Li and Z. Xu}
% First names are abbreviated in the running head.
% If there are more than two authors, 'et al.' is used.
%
\institute{LIACS, Leiden University, Leiden, Netherlands\\
\email{\{s3430863, s3649024\}@umail.leidenuniv.nl}}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
%The abstract should briefly summarize the contents of the paper in 150--250 words.
In this project, we present an ensemble learning method for sentiment classification. We use the state-of-the-art BERT as base models and a soft voting classifier as the meta model. The effectiveness of our proposed approach is verified on SemEval-2017 dataset, which quantifies the sentiment in tweets via a three-point ordinal scale. Experimental results show that the overall performance of our proposed model is better than that of baselines.

\keywords{Sentiment Analysis \and BERT \and Ensemble Learning}
\end{abstract}
%
%
%
\section{Introduction}
Sentiment analysis, a growing field today, is the process of analyzing pieces of writing to determine the emotional tone they carry. In simple words, sentiment analysis helps to find the author's attitude. This method can be used by businesses to analyze product reviews and feedback, especially for social media companies with large information streams because of the wealth of data they generate. Researchers also have a special interest in social media data because of their easy availability and rapid change.

SemEval is an International Workshop on Semantic Evaluation, formerly SensEval. It is an ongoing series of evaluations of computational semantic analysis systems. The SemEval-2017 Task 4 focuses on classifying and quantifying the sentiment of tweets. The task was included in this workshop in the previous year \cite{nakov-etal-2016-semeval}, and Sentiment Analysis in Twitter has been run yearly since 2013 \cite{nakov-etal-2013-semeval}. The subtask A of this task is the problem we try to unravel in this article. It is about deciding the overall sentiment of tweets and marking them on a three-point ordinal scale.% In this task, we are interested in the sentiment expressed in individual tweets. This is not from the overall perspective of political science, economics, and social science, but from a microscopic aspect of language processing.

Our proposed model adopts two strategies with the goal of achieving a higher accuracy. One is choosing more powerful BERT and its variants as base models instead of CNN, LSTM, or SVM. The other is that we design an ensemble learning approach to integrate the advantages of different BERT models, which encourages the learning diversity and might enhance model performance. The classification task is divided into two stages. In the first stage, we train a series of base models to generate corresponding predictions. These predictions are the inputs of the meta model in the second stage. The final predictions are the outputs of the meta model.

The remainder of the paper is structured as follows. Section 2 discusses previous work related to sentiment classification. Following that, Section 3 introduces datasets provided by SemEval. We explain the data preparation workflow, our proposed approach, and baselines in Section 4. Section 5 describes the procedure of experiments in detail and reports the experimental results. The paper is concluded with a discussion about results and a brief summary. We also attach the contributions of group members at the end of the paper. 

\section{Related Work}
The prevalence of social media has been driving the growth of sentiment analysis research since early 2000 \cite{2018sa}. Earlier studies mainly focused on the lexicon-based approach, which discovers sentimental polarity based either on a predefined dictionary or on some semantic methods \cite{2014sa}. Previous works have also intensively researched the applications of traditional machine learning methods in sentiment analysis, such as SVM, naive Bayes, etc \cite{2019sa}. However, both lexicon-based and machine learning approaches require manually feature selection, which limits their applicability in large and dynamic user-generated content like tweets.

In recent years, deep learning methods have been introduced into sentiment analysis field and have achieved the remarkable performance on many benchmark datasets \cite{2019sa}. Popular deep learning frameworks for sentiment classification include CNNs and RNNs. Inspired by computer vision field, the CNN architecture first constructs a sentence matrix using tokens and word vectors. Then it treats the sentence matrix as an "image" and applys regular convolution operations on the matrix \cite{cnns}. The major drawback of CNNs is that CNNs can't capture the hierarchical relationship between local features. Besides, the pooling layer may cause the loss of critical information and harm the classification accuracy. LSTMs and GRUs are two commonly used RNN variants. Nowak et al. \cite{rnns} compared LSTM, bidirectional LSTM, and GRU with other algorithms on real-world datasets and found RNNs outperformed baselines in sentiment classification task. However, RNNs are not suitable for processing long sequences and the training process is very slow. There were also attempts to combine CNNs and RNNs, aiming at enhancing model performance further \cite{com1}\cite{com2}. But these hybrid models don't overcome the intrinsic weaknesses of CNNs and RNNs.

In this project, we consider BERT, the state-of-the-art model in several natural language processing tasks. Compared to CNNs and RNNs, BERT leverages the attention mechanism to model the complex relationship between features in short-distance as well as long-distance context. And it is efficient on GPUs and TPUs \cite{bert}. We describe our method in detail in Section \ref{sec:method}.

\section{Data}
The dataset consists of 11 files with tweets from 2013 to 2015. Tweets are marked with sentiment labels on a three-point scale $\left\{ \text{Positive, Neutral, Negative} \right\}$. Each tweet corresponds to one row in datasets following a fixed format: [\texttt{id}, \texttt{sentiment label}, \texttt{text}].

The criterion of tweet selection is covering popular topics at the time of sending tweets. Datasets are downloaded via the Twitter API and have been preprocessed by workshop in advance, where three kinds of data have been removed: repeated tweets, the bag-of-words cosine similarity exceeded 0.6, and topics with less than 100 tweets. CrowdFlower is used to create all annotations on both training set and test set. Each tweet is annotated by at least five people to ensure the accuracy of annotations. Another main quality control measure is performing hidden tests to filter out unqualified annotations. There are also manual inspections on pilot runs aiming at adjusting the annotation instructions dynamically. Table \ref{tab:3-stat} summarizes the descriptive statistics of 11 files.

\begin{table}[!ht]
	\centering
	\caption{Descriptive Statistics of Datasets} 
    \label{tab:3-stat}
	\begin{tabular}{lllll}
		\toprule
        \textbf{Dataset} & \textbf{Positive} & \textbf{Neutral} & \textbf{Negative} & \textbf{Total}\\ 
        \midrule
		2013train   & 3,640     & 4,586    & 1,458     & 9,684  \\
		2013dev     & 575      & 739     & 340      & 1,654  \\
		2013test    & 1,475     & 1,513    & 559      & 3,547  \\
		2014sarcasm & 20       & 7       & 22       & 49    \\
		2014test    & 982      & 669     & 202      & 1,853  \\
		2015train   & 170      & 253     & 66       & 489   \\
		2015test    & 1,038     & 987     & 365      & 2,390 \\
		2016train   & 3,017     & 2,001    & 850      & 5,868  \\
		2016dev     & 829      & 746     & 391      & 1,966  \\
		2016devtest & 994      & 681     & 325      & 2,000  \\
		2016test    & 7,059     & 10,342   & 3,231     & 20,632 \\ 
        \midrule
        Total   & 19,799  & 22,524  & 7,809  &   50,132\\
        \bottomrule
	\end{tabular}
\end{table}

\section{Methodology} \label{sec:method}
\subsection{Data Preparation}
We perform data cleaning and dataset division before modeling. Notice that records in 2016test file have a redundant \texttt{\textbackslash t} before \texttt{\textbackslash n}. We remove the unnecessary \texttt{\textbackslash t} to ensure the dataset would be read into Python appropriately. To create training set and test set, we concatenate all 11 files and randomly sample 80\% records as training set while treating the others as test set. As a result, there are 40,105 records in training set and 10,027 records in test set. We also create the validation set for hyperparameter tuning by randomly sampling 20\% recoreds from the training set. The random seed is set to 42 in our experiments to ensure the reproducibility.

\subsection{BERT}
BERT \cite{bert} is a popular NLP model that has achieved the remarkble performance on various tasks, such as text classification, question answering, etc. Compared to traditional RNN models, it encodes sequences in both directions instead of following a left-to-right or right-to-left routine, which is more closer to how humans understand the meaning of the text. Its bidirectional encoding ability is accomplished by the transformer architecture, which is a stack of encoders using multi-head attention mechanism. Specifically, encoders process the entire sequence at once and use layer-wise tensor operations to learn relationships between words in a sentence. This design not only encodes the inputs bidirectionally but also eliminates the possible local bias, for it gives equal importance to the local context and the long-distance context. It is worth mentioning that the training process of BERT is more efficient than RNN due to the feasibility of parallelization.

BERT requires a special format of input called WordPiece. The WordPiece tokenizer splits words into tokens and adds special tokens at the beginning as well as the end of the sentence. Preprocessed inputs provide three aspects of information for BERT model: tokens, sentence segments, and the absolute position. We perform the tokenization operation on both the training set and the test set before modeling.

We also consider two variants of BERT in our project -- RoBERTa \cite{roberta} and DistilBERT \cite{distilbert}. RoBERTa is an optimized version of BERT model. Authors of RoBERTa find that the original BERT is actually undertrained after reproducing BERT experiments. Their solution is increasing the training epochs of BERT and carefully select hyperparameters, which significantly improves the model performance in practice. On the contrary, DistilBERT is a compressed version of BERT. The main idea of DistilBERT is reducing the model size via distillation technique. Distillation consists of a teacher model and a student model. The teacher model is trained on a large dataset and is fine-tuned to maximize the accuracy. However, many features learned by the teacher model are redundant for a specific task. Therefore, we can train a much smaller student model to focus on key features and imitate the output of the teacher model. Experimental results show that DistilBERT is cheaper to train while maintaining a comparable performance to BERT.

\subsection{Ensemble Learning}
Ensemble learning refers to methods that combine multiple models to achieve better performance in machine learning. It encourages base models to learn different aspects of the data to reduce errors and avoid being entrapped in local optima. In the project, sentiment analysis in Twitter is a multi-class classification problem. And we develop a classification voting ensemble model integrated BERT and its variants. Figure \ref{fig:model} illustrates the architecture of our model.

\begin{figure}[!ht]
    \centering
    \includegraphics[width=11cm]{./figs/model.png}
    \caption{The Architecture of the Proposed Model}
    \label{fig:model}
\end{figure}

Each base model outputs the class prediction for each record. Here we consider two strategies for the meta model --- hard voting and soft voting. Hard voting means the voting classifier adopts the plurality voting strategy to generate the final prediction. In other words, the final prediction is the class label received the majority votes from base models. If all classes have the same votes, the voting classifier will choose a class label randomly as the output. However, hard voting ignores the probability information related to each model's prediction and is likely to be affected by relatively weaker base models. Thus, we set the hard voting ensemble learning as one of our baselines and adopt the soft voting strategy.

Soft voting strategy averages the output probabilities of base models and selects the class with the highest probability. However, outputs of base models may contain negative numbers and are not normalized. We convert the outputs into a probability distribution by applying the softmax function:
\begin{align*}
    P(c_i) = \frac{\exp{(\hat{y}_i)}}{\sum_i \exp{(\hat{y}_i)}}
\end{align*}
where $P(c_i)$ is the estimated probability of class $i$ and $\hat{y}_i$ is the output value of the base model for class $i$. In summary, our proposed model is the ensemble of three BERT base learners and a soft voting classifier.

\subsection{Baselines}
We compare the proposed method against four baselines. The first three benchmarks are BERT, RoBERTa, and DistilBERT alone. We set the ensemble BERT model with hard voting strategy as the fourth baseline, as mentioned in Section 4.3. Accuracy, macro recall, and macro F1 are selected as metrics in experiments.

\section{Experiments}
\subsection{Experimental Setup}
We replace three patterns in tweets and tokenize the textual data before feeding them into models. The first pattern to be substituted is $@user\_name$, which indicates other users in Twitter. The second is to replace $\&amp$ with $\&$. And the last is to delete redundant spaces. We also map the text labels into categorical variables to meet the requirements of APIs, i.e., setting Negative as $0$, Neutral as $1$, and Positive as $2$. We apply Hugging Face's implementation of BERT models. Table \ref{tab:exp-software} summarizes the software environment of our experiments. 

\begin{table}[!ht]
    \centering
    \caption{Summary of Software Environment}
    \label{tab:exp-software}
    \begin{tabular}{lll}
        \toprule
        \textbf{Item} & \textbf{Version} & \textbf{Function}\\
        \midrule
        Python & 3.9 & Programming language.\\
        Transformers & 4.25.1 & Transformer models in Hugging Face.\\
        TensorFlow & 2.11.0 & Deep learning APIs.\\
        scikit-learn & 1.2.0 & Preprocessing and evaluation.\\
        \bottomrule
    \end{tabular}
\end{table}

Our experimental process consists of hyperparameter tuning, training, and evaluation. Grid search technique is used to optimize hyperparameters. The best hyperparameter setting is \texttt{batch size}$= 16$, \texttt{initial learning rate}$ = 10^{-5}$. We combine early stopping and model checkpoint saving strategies to enhance the model performance further. Specifically, we set the number of epochs as 50, stop the training if the accuracy doesn't improve in 3 epoches, and load the best model when evaluating. All experiments is deployed on a cloud server with an Intel Xeon(R) Gold 6330 CPU and a RTX A5000 GPU.

\subsection{Results}
Table \ref{tab:results} reports the experimental results. The detailed evaluation report for each class is attached as Appendix.

\begin{table}[!ht]
	\centering
	\caption{Summary of Results}
	\label{tab:results}
	\begin{tabular}{llll}
		\toprule
		\textbf{Model}       & \textbf{Precision} & \textbf{Macro Recall} & \textbf{Macro F1} \\ 
		\midrule
		BERT        & 0.7338     & 0.7169  & 0.7181    \\
		RoBERTa     & 0.7473     & 0.7236  & 0.7310    \\
		DistilBERT  & 0.7226     & 0.6920  & 0.7002    \\
		Ensemble BERT with Hard Voting & 0.7463     & 0.7233  & 0.7293    \\
        Ensemble BERT with Soft Voting & \textbf{0.7510}     & \textbf{0.7305}  & \textbf{0.7347}    \\ 
		\bottomrule
	\end{tabular}
\end{table}

Our proposed model outperforms benchmarks on all metrics, which supports the effectiveness of our method. For the base model, standard BERT performs worse than RoBERTa but better than DistilBERT, which is consistent with the theory. It is worth noting that the ensemble BERT model with hard voting achieves slighly worse scores on all metrics than RoBERTa alone. We think the reason is that the performance of hard voting in this task is affected by DistilBERT that is a weaker model compared to the other two.

\section{Discussion}
The soft-voting meta model outperforms the four baselines, but there are still some improvements that could be done. 

First, the recall of the soft voting model is relatively low, and the recall of the negative sentiment is only $0.59$. Besides, the running of the BERT is very time-consuming, and the efficiency can be improved through parallel. Third, the ensemble learning model is influenced by the classifier. Constrained by computational resources, all of our baseline models use the base-cased classifier. And it may observe better results implementing the large and uncased classifier.


\section{Conclusion}
We build an ensemble BERT model using soft voting to classify the sentiments of the Twitter text. And this model is quite powerful compared to three BERT base models and the soft voting ensemble model with an accuracy of $0.752$.
 
 
\section{Contributions}
Authors contribute equally to the programming and the report.

% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{paper}
%

\section*{Appendix}

\end{document}
